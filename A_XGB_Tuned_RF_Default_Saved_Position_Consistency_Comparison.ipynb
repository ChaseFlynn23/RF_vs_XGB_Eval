{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed7ae50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import AA_Import_LCP_Functions as chase_lcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea501d2",
   "metadata": {},
   "source": [
    "# Data Import & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dad0c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lccdata_folder = 'lccdata_files'\n",
    "\n",
    "# Import LCC data files for wild type protein and mutant protein\n",
    "wt_dict = chase_lcc.import_lcc_data(lccdata_folder, 'w')\n",
    "D132H_dict = chase_lcc.import_lcc_data(lccdata_folder, 'm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95184421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def prepare_data_x(wt_dict, D132H_dict, window_size):\n",
    "    wildtype_data = wt_dict[window_size]\n",
    "    wildtype_label = np.zeros(len(wildtype_data))\n",
    "    mutant_data = D132H_dict[window_size]\n",
    "    mutant_label = np.ones(len(mutant_data))\n",
    "\n",
    "    lcc_data = np.vstack((wildtype_data, mutant_data))\n",
    "    label_data = np.hstack((wildtype_label, mutant_label))\n",
    "    lcc_data, label_data = unison_shuffled_copies(lcc_data, label_data)\n",
    "    lcc_data /= 100\n",
    "    upper_training_limit = int(len(lcc_data) * 0.8)\n",
    "    \n",
    "    # Splitting the data into training and testing sets\n",
    "    X_train, X_test = lcc_data[:upper_training_limit], lcc_data[upper_training_limit:]\n",
    "    y_train, y_test = label_data[:upper_training_limit], label_data[upper_training_limit:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c3cb6a",
   "metadata": {},
   "source": [
    "## XGBoost Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd24d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the mode of hyperparameter values\n",
    "def get_mode_hyperparameters(trial_number):\n",
    "    path = f'XGB_Tuning/XGB_Tuning_Trial_{trial_number}/tuning_results.json'\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"No tuning results found for trial number {trial_number}\")\n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "        tuning_results = json.load(file)\n",
    "    \n",
    "    def custom_mode(values):\n",
    "        if not values:\n",
    "            return None\n",
    "        return max(set(values), key=values.count)\n",
    "    \n",
    "    mode_hyperparameters = {\n",
    "        'eta': custom_mode(list(tuning_results['best_eta_values'].values())),\n",
    "        'max_depth': custom_mode(list(tuning_results['best_max_depth_values'].values())),\n",
    "        'subsample': custom_mode(list(tuning_results['best_subsample_values'].values())),\n",
    "    }\n",
    "    \n",
    "    return mode_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44eaf4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted function to run trials with the option to use tuned hyperparameters for XGBoost\n",
    "def run_trials(n_trials, classifier, classifier_name, window_sizes, lccdata_folder, use_tuned_params=False, trial_number=None):\n",
    "    consistency_counts = {}\n",
    "    \n",
    "    # Load mode hyperparameters if using tuned XGBoost\n",
    "    if use_tuned_params and classifier_name == \"XGB_Tuned\":\n",
    "        if trial_number is None:\n",
    "            raise ValueError(\"Trial number must be provided for XGB_Tuned.\")\n",
    "        mode_hyperparameters = get_mode_hyperparameters(trial_number)\n",
    "    else:\n",
    "        mode_hyperparameters = None\n",
    "\n",
    "    for trial in tqdm(range(1, n_trials + 1), desc=f\"{classifier_name} Trials\"):\n",
    "        print(f\"\\nStarting {classifier_name} Trial {trial}\")\n",
    "        trial_dir = f\"{classifier_name}_Consistency_Trial_{trial}\"\n",
    "        os.makedirs(trial_dir, exist_ok=True)\n",
    "\n",
    "        for window_size in window_sizes:\n",
    "            X_train, X_test, y_train, y_test = prepare_data_x(wt_dict, D132H_dict, window_size)\n",
    "            \n",
    "            if use_tuned_params and classifier_name == \"XGB_Tuned\":\n",
    "                clf = classifier(**mode_hyperparameters)\n",
    "            else:\n",
    "                clf = classifier()\n",
    "            \n",
    "            clf.fit(X_train, y_train)\n",
    "            feature_importances = clf.feature_importances_\n",
    "            trial_filename = os.path.join(trial_dir, f\"Feature_Importance_WS_{window_size}.csv\")\n",
    "            pd.DataFrame(feature_importances).to_csv(trial_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76926a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "XGB_Tuned Trials:   0%|                                   | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting XGB_Tuned Trial 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "XGB_Tuned Trials:  20%|█████▏                    | 1/5 [02:49<11:16, 169.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting XGB_Tuned Trial 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "XGB_Tuned Trials:  40%|██████████▍               | 2/5 [05:38<08:27, 169.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting XGB_Tuned Trial 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "XGB_Tuned Trials:  60%|███████████████▌          | 3/5 [08:27<05:38, 169.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting XGB_Tuned Trial 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "XGB_Tuned Trials:  80%|████████████████████▊     | 4/5 [11:16<02:49, 169.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting XGB_Tuned Trial 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XGB_Tuned Trials: 100%|██████████████████████████| 5/5 [14:05<00:00, 169.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# Example usage for running trials with tuned XGBoost hyperparameters\n",
    "n_trials = 5\n",
    "window_sizes = range(2, 52)\n",
    "trial_number_for_tuned_hyperparameters = 1\n",
    "\n",
    "run_trials(n_trials, XGBClassifier, 'XGB_Tuned', window_sizes, lccdata_folder, use_tuned_params=True, trial_number=trial_number_for_tuned_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c42d1fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB_Tuned Consistency:\n",
      "1/5 Trials: 9 positions\n",
      "2/5 Trials: 10 positions\n",
      "3/5 Trials: 11 positions\n",
      "4/5 Trials: 8 positions\n",
      "5/5 Trials: 143 positions\n",
      "RF Consistency:\n",
      "1/5 Trials: 17 positions\n",
      "2/5 Trials: 14 positions\n",
      "3/5 Trials: 14 positions\n",
      "4/5 Trials: 19 positions\n",
      "5/5 Trials: 160 positions\n"
     ]
    }
   ],
   "source": [
    "# Feature selection consistency comparison\n",
    "models = ['XGB_Tuned', 'RF']\n",
    "n_trials = 5\n",
    "window_sizes = range(2, 52)\n",
    "threshold = 0.0285\n",
    "results = {model: {i: 0 for i in range(1, 6)} for model in models}\n",
    "\n",
    "def weighted_threshold(window_size):\n",
    "    return threshold * (68 / (70 - window_size))\n",
    "\n",
    "def count_positions_above_threshold(model, window_size, threshold):\n",
    "    positions_above_threshold = []\n",
    "    for trial in range(1, n_trials + 1):\n",
    "        folder_name = f'{model}_Consistency_Trial_{trial}'\n",
    "        file_name = f'Feature_Importance_WS_{window_size}.csv'\n",
    "        file_path = os.path.join(folder_name, file_name)\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            positions_above_threshold.append(df[df['0'] > threshold].index.tolist())\n",
    "        else:\n",
    "            # Handle missing file if necessary\n",
    "            pass\n",
    "    \n",
    "    # Count positions saved in all 5, 4, 3, 2, and 1 trials\n",
    "    all_positions = set(sum(positions_above_threshold, []))\n",
    "    for position in all_positions:\n",
    "        counts = sum(position in trial_positions for trial_positions in positions_above_threshold)\n",
    "        results[model][counts] += 1\n",
    "\n",
    "for model in models:\n",
    "    for window_size in window_sizes:\n",
    "        w_threshold = weighted_threshold(window_size)\n",
    "        count_positions_above_threshold(model, window_size, w_threshold)\n",
    "\n",
    "# Results\n",
    "for model in models:\n",
    "    print(f'{model} Consistency:')\n",
    "    for k, v in results[model].items():\n",
    "        print(f'{k}/5 Trials: {v} positions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc13ac6a",
   "metadata": {},
   "source": [
    "# Consistency Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09849923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Stability Index: 0.3772\n",
      "RF Stability Index: 0.4012\n"
     ]
    }
   ],
   "source": [
    "def calculate_stability_index(consistency_counts):\n",
    "    \"\"\"\n",
    "    Calculate the stability index based on the consistency counts for different levels.\n",
    "    \n",
    "    Args:\n",
    "    - consistency_counts (dict): A dictionary where keys are the number of trials (1 to 5)\n",
    "      a feature appears in, and values are the number of positions that appear in that many trials.\n",
    "      \n",
    "    Returns:\n",
    "    - float: The calculated stability index.\n",
    "    \"\"\"\n",
    "    # Define weights for each consistency level\n",
    "    weights = {1: 1, 2: 2, 3: 3, 4: 4, 5: 5}\n",
    "    \n",
    "    # Calculate the weighted sum of positions for each consistency level\n",
    "    weighted_sum = sum(weights[key] * count for key, count in consistency_counts.items())\n",
    "    \n",
    "    # Calculate the total possible weighted sum, assuming all positions were selected in all trials\n",
    "    max_weighted_sum = sum(weights[key] * max(consistency_counts.values()) for key in weights)\n",
    "    \n",
    "    # Calculate the stability index as the ratio of the weighted sum to the maximum possible weighted sum\n",
    "    stability_index = weighted_sum / max_weighted_sum\n",
    "    \n",
    "    return stability_index\n",
    "\n",
    "# Consistency data for XGBoost and RF\n",
    "xgb_consistency = {1: 9, 2: 10, 3: 11, 4: 8, 5: 143}\n",
    "rf_consistency = {1: 17, 2: 14, 3: 14, 4: 19, 5: 160}\n",
    "\n",
    "# Calculate stability indexes\n",
    "xgb_stability_index = calculate_stability_index(xgb_consistency)\n",
    "rf_stability_index = calculate_stability_index(rf_consistency)\n",
    "\n",
    "print(f\"XGBoost Stability Index: {xgb_stability_index:.4f}\")\n",
    "print(f\"RF Stability Index: {rf_stability_index:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f62917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Consistency Score: 4.4696\n",
      "RF Consistency Score: 4.2991\n"
     ]
    }
   ],
   "source": [
    "def calculate_consistency_score(consistency_counts):\n",
    "    \"\"\"\n",
    "    Calculate the consistency score for feature selection across trials.\n",
    "    \n",
    "    Args:\n",
    "    - consistency_counts (dict): A dictionary where keys are the number of trials (1 to 5)\n",
    "      a feature appears in, and values are the number of positions that appear in that many trials.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The calculated consistency score.\n",
    "    \"\"\"\n",
    "    # Initialize total counts and weighted sum\n",
    "    total_counts = sum(consistency_counts.values())\n",
    "    weighted_sum = sum(key * value for key, value in consistency_counts.items())\n",
    "    \n",
    "    # Calculate the consistency score as the weighted sum of positions divided by the total counts\n",
    "    consistency_score = weighted_sum / total_counts if total_counts else 0\n",
    "    \n",
    "    return consistency_score\n",
    "\n",
    "# Consistency data for XGBoost and RF\n",
    "xgb_consistency = {1: 9, 2: 10, 3: 11, 4: 8, 5: 143}\n",
    "rf_consistency = {1: 17, 2: 14, 3: 14, 4: 19, 5: 160}\n",
    "\n",
    "# Calculate consistency scores\n",
    "xgb_consistency_score = calculate_consistency_score(xgb_consistency)\n",
    "rf_consistency_score = calculate_consistency_score(rf_consistency)\n",
    "\n",
    "print(f\"XGBoost Consistency Score: {xgb_consistency_score:.4f}\")\n",
    "print(f\"RF Consistency Score: {rf_consistency_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
